# Pr√©vision de la Demande √ânerg√©tique pour Smart Grid via LSTM

## Contexte du projet

La start-up **EcoVolt** op√®re un r√©seau intelligent de distribution d‚Äô√©lectricit√© (*Smart Grid*) destin√© notamment √† l‚Äôalimentation de bornes de recharge pour v√©hicules √©lectriques.

Afin d‚Äôanticiper les pics de consommation et d‚Äô√©viter les risques de surcharge du r√©seau, EcoVolt souhaite mettre en place un **mod√®le de pr√©vision √† court terme** capable de :

> **Pr√©dire la consommation √©lectrique de l‚Äôheure suivante (t+1)**  
> √† partir des **24 heures pr√©c√©dentes**, en tenant compte du **mix √©nerg√©tique**.

Ce projet constitue une **preuve de concept (POC)** bas√©e sur des donn√©es historiques horaires.

---

##  Objectifs du projet

### Objectif m√©tier
- Anticiper les variations de la demande √©nerg√©tique
- Aider √† la prise de d√©cision pour la gestion du r√©seau √©lectrique

### Objectifs p√©dagogiques
- Comprendre le fonctionnement des **r√©seaux de neurones r√©currents (RNN)**
- Impl√©menter un **mod√®le LSTM multivari√©**
- Travailler sur un **probl√®me r√©el de s√©ries temporelles**
- Justifier les choix de **pr√©processing**, d‚Äô**architecture** et d‚Äô**√©valuation**
- Analyser et interpr√©ter les r√©sultats obtenus

---

##  Probl√©matique IA

- **Type de probl√®me** : R√©gression supervis√©e sur s√©rie temporelle
- **Entr√©e (X)** : S√©quence de 24 heures cons√©cutives
- **Sortie (y)** : Consommation √©lectrique √† l‚Äôheure suivante
- **Mod√®le utilis√©** : LSTM (Long Short-Term Memory)

---

##  Description du dataset

Le dataset est constitu√© de donn√©es **horaires** et contient les colonnes suivantes :

###  Temps
- `DateTime` : Date et heure de la mesure

### Variable cible
- `Consumption` : Consommation √©lectrique totale (MW)

###  Variables explicatives (production)
- `Production`
- `Nuclear`
- `Wind`
- `Solar`
- `Hydroelectric`
- `Coal`
- `Oil and Gas`
- `Biomass`

=> Toutes les variables sont **num√©riques**, ce qui les rend directement exploitables par un mod√®le LSTM.

---

## √âtapes techniques du projet

### 1Ô∏è. Pr√©paration des donn√©es temporelles

- Conversion de la colonne `DateTime` en format datetime
- Tri chronologique des donn√©es
- Mise en index temporel

 **Pourquoi ?**
- L‚Äôordre temporel est critique en s√©ries temporelles
- Les donn√©es ne doivent jamais √™tre m√©lang√©es (pas de shuffle)
- Cela √©vite toute fuite de donn√©es du futur vers le pass√©

---

### 2Ô∏è. Normalisation des donn√©es

- Utilisation de **MinMaxScaler**
- Mise √† l‚Äô√©chelle des variables entre 0 et 1

 **Justification**
- La normalisation est indispensable en Deep Learning
- Elle am√©liore la stabilit√© et la convergence du mod√®le
- MinMaxScaler est particuli√®rement adapt√© aux fonctions d‚Äôactivation `tanh` et `sigmoid` utilis√©es dans les LSTM

---

### 3Ô∏è. Cr√©ation des s√©quences (Windowing)

Transformation des donn√©es tabulaires en donn√©es s√©quentielles :

- Fen√™tre glissante de **24 heures**
- Chaque √©chantillon devient une s√©quence temporelle

 **Format attendu par le LSTM** :
 (samples, timesteps, features)

 
- `samples` : nombre de s√©quences
- `timesteps` : 24 heures
- `features` : nombre de variables √©nerg√©tiques

---

### 4Ô∏è. Split Train / Test temporel

- S√©paration chronologique des donn√©es
- 80 % pour l‚Äôentra√Ænement
- 20 % pour le test

 **Pourquoi pas de train_test_split classique ?**
- Pour √©viter toute fuite de donn√©es temporelles
- Pour simuler un sc√©nario r√©el de mise en production

---

### 5Ô∏è. Architecture du mod√®le LSTM

Architecture utilis√©e :

- 1 couche **LSTM (64 unit√©s)**
- 1 couche **Dropout** pour limiter le surapprentissage
- 1 couche **Dense** pour la pr√©diction finale

**Choix techniques**
- 64 unit√©s permettent de capturer les d√©pendances temporelles sans surcomplexifier le mod√®le
- Une seule couche LSTM est suffisante pour un POC
- Optimiseur : Adam
- Fonction de perte : Mean Squared Error (MSE)

---

### 6Ô∏è. Entra√Ænement du mod√®le

- Nombre d‚Äô√©poques : 30
- Batch size : 32
- Validation split : 20 %

 Suivi de l‚Äôapprentissage via les courbes :
- `loss` (entra√Ænement)
- `val_loss` (validation)

---

### 7Ô∏è. √âvaluation du mod√®le

#### Courbes loss / val_loss
- Diminution progressive et parall√®le
- Absence de divergence
- Pas de surapprentissage d√©tect√©

####  Graphe Valeurs R√©elles vs Valeurs Pr√©dites
- Forte superposition des courbes
- Bonne capture des tendances et cycles journaliers
- Erreurs principalement sur des pics extr√™mes (√©v√©nements atypiques)

---

##  R√©sultats et interpr√©tation

- Le mod√®le g√©n√©ralise correctement sur le jeu de test
- Les pr√©dictions sont coh√©rentes avec la r√©alit√© m√©tier
- Le LSTM capte efficacement les d√©pendances temporelles
- Le mod√®le est **exploitable dans un contexte Smart Grid**

---


---

##  Technologies utilis√©es

- Python
- Pandas, NumPy
- Scikit-learn
- TensorFlow / Keras
- Matplotlib
- Google Colab

---

## üë§ Auteur

**Ayoub Motei**  


---


